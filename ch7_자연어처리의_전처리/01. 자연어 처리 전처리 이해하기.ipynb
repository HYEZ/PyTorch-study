{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. 자연어 처리 전처리 이해하기\n",
    "자연어 처리는 일반적으로 `토큰화`, `단어 집합 생성`, `정수 인코딩`, `패딩`, `벡터화`의 과정을 거칩니다. 이번 챕터에서는 이러한 전반적인 과정에 대해서 이해합니다.\n",
    "\n",
    "## 1. 토큰화(Tokenization)\n",
    "__주어진 텍스트를 단어 또는 문자 단위로 자르는 것을 토큰화라고 합니다.__ 예를 들어 주어진 문장이 다음과 같다고 해봅시다. 영어의 경우 토큰화를 사용하는 도구로서 대표적으로 spaCy와 NLTK가 있습니다. 물론, 파이썬 기본 함수인 split으로 토큰화를 할 수도 있습니다.\n",
    "\n",
    "우선 영어에 대해서 토큰화 실습을 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text = \"A Dog Run back corner near spare bedrooms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. spaCy 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(en_text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(en_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(en_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. NLTK 사용하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ohyeji/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(en_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 띄어쓰기로 토큰화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "print(en_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사실 영어의 경우에는 띄어쓰기 단위로 토큰화를 해도 단어들 간 구분이 꽤나 명확하기 때문에, 토큰화 작업이 수월합니다. __하지만 한국어의 경우에는 토큰화 작업이 훨씬 까다롭습니다. 그 이유는 한국어는 조사, 접사 등으로 인해 단순 띄어쓰기 단위로 나누면 같은 단어가 다른 단어로 인식되어서 단어 집합(vocabulary)의 크기가 불필요하게 커지기 때문입니다.__\n",
    "\n",
    "- __`단어 집합(vocabuary)`이란 중복을 제거한 텍스트의 총 단어의 집합(set)을 의미합니다.__\n",
    "\n",
    "예를 들어 단어 '사과'가 많이 들어간 어떤 문장에 띄어쓰기 토큰화를 한다면 '사과가', '사과를', '사과의', '사과와', '사과는'과 같은 식으로 같은 단어임에도 조사가 붙어서 다른 단어로 인식될 수 있습니다. 예를 통해 구체적으로 이해해봅시다.\n",
    "\n",
    "### 4. 한국어 띄어쓰기 토큰화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사과의', '놀라운', '효능이라는', '글을', '봤어.', '그래서', '오늘', '사과를', '먹으려고', '했는데', '사과가', '썩어서', '슈퍼에', '가서', '사과랑', '오렌지', '사왔어']\n"
     ]
    }
   ],
   "source": [
    "print(kor_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예제에서는 '사과'란 단어가 총 4번 등장했는데 모두 '의', '를', '가', '랑' 등이 붙어있어 이를 제거해주지 않으면 기계는 전부 다른 단어로 인식하게 됩니다.\n",
    "\n",
    "### 5. 형태소 토큰화\n",
    "위와 같은 상황을 방지하기 위해서 __한국어는 보편적으로 `형태소 분석기`로 토큰화를 합니다.__ 여기서는 형태소 분석기 중에서 mecab을 사용해보겠습니다. 아래의 커맨드로 colab에서 mecab을 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
      "remote: Enumerating objects: 75, done.\u001b[K\n",
      "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
      "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
      "remote: Total 75 (delta 33), reused 20 (delta 5), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (75/75), done.\n",
      "/Users/ohyeji/Desktop/연구실/스터디/PyTorch로_시작하는_딥러닝/ch7_자연어처리의_전처리/Mecab-ko-for-Google-Colab\n",
      "install_mecab-ko_on_colab190912.sh: line 4: cd: /content: No such file or directory\n",
      "Installing konlpy.....\n",
      "Requirement already satisfied: konlpy in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from konlpy) (4.6.2)\n",
      "Requirement already satisfied: colorama in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from konlpy) (3.9.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from konlpy) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from konlpy) (1.19.4)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (2.25.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Done\n",
      "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
      "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
      "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
      "install_mecab-ko_on_colab190912.sh: line 15: wget: command not found\n",
      "Done\n",
      "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
      "Done\n",
      "Change Directory to mecab-0.996-ko-0.9.2.......\n",
      "install_mecab-ko_on_colab190912.sh: line 23: cd: mecab-0.996-ko-0.9.2/: No such file or directory\n",
      "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
      "configure\n",
      "make\n",
      "make check\n",
      "make install\n",
      "ldconfig\n",
      "Done\n",
      "Change Directory to /content\n",
      "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
      "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
      "install_mecab-ko_on_colab190912.sh: line 44: wget: command not found\n",
      "Done\n",
      "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
      "Done\n",
      "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
      "install_mecab-ko_on_colab190912.sh: line 52: cd: mecab-ko-dic-2.1.1-20180720/: No such file or directory\n",
      "Done\n",
      "installing........\n",
      "configure\n",
      "make\n",
      "make install\n",
      "apt-get update\n",
      "apt-get upgrade\n",
      "apt install curl\n",
      "apt install git\n",
      "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
      "Done\n",
      "Successfully Installed\n",
      "Now you can use Mecab\n",
      "from konlpy.tag import Mecab\n",
      "mecab = Mecab()\n",
      "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
      "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
      "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "%cd Mecab-ko-for-Google-Colab\n",
    "!bash install_mecab-ko_on_colab190912.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사과', '의', '놀라운', '효능', '이', '라는', '글', '을', '봤', '어', '.', '그래서', '오늘', '사과', '를', '먹', '으려고', '했', '는데', '사과', '가', '썩', '어서', '슈퍼', '에', '가', '서', '사과', '랑', '오렌지', '사', '왔', '어']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "print(tokenizer.morphs(kor_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞선 예와 다르게 '의', '를', '가', '랑' 등이 전부 분리되어 기계는 '사과'라는 단어를 하나의 단어로 처리할 수 있습니다.\n",
    "\n",
    "지금까지는 단어 또는 형태소 단위로 토큰화를 했지만 이보다도 더 작은 단위인 문자 단위로 토큰화를 수행하는 경우도 있습니다.\n",
    "\n",
    "### 6. 문자 토큰화\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', ' ', 'D', 'o', 'g', ' ', 'R', 'u', 'n', ' ', 'b', 'a', 'c', 'k', ' ', 'c', 'o', 'r', 'n', 'e', 'r', ' ', 'n', 'e', 'a', 'r', ' ', 's', 'p', 'a', 'r', 'e', ' ', 'b', 'e', 'd', 'r', 'o', 'o', 'm', 's']\n"
     ]
    }
   ],
   "source": [
    "print(list(en_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단히 토큰화에 대해서 알아봤습니다. 이제부터는 좀 더 많은 데이터를 가지고 실습해보겠습니다.\n",
    "\n",
    "***\n",
    "\n",
    "## 2. 단어 집합(Vocabulary) 생성\n",
    "__`단어 집합(vocabuary)`이란 중복을 제거한 텍스트의 총 단어의 집합(set)을 의미합니다.__ 우선, 실습을 위해서 깃허브에서 '네이버 영화 리뷰 분류하기' 데이터를 다운로드하겠습니다. 네이버 영화 리뷰 데이터는 총 20만 개의 영화 리뷰를 긍정 1, 부정 0으로 레이블링한 데이터입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2190435</td>\n",
       "      <td>사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9279041</td>\n",
       "      <td>완전 감동입니다 다시봐도 감동</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7865729</td>\n",
       "      <td>개들의 전쟁2 나오나요? 나오면 1빠로 보고 싶음</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7477618</td>\n",
       "      <td>굿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9250537</td>\n",
       "      <td>바보가 아니라 병 쉰 인듯</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
       "5   2190435                      사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화      1\n",
       "6   9279041                                   완전 감동입니다 다시봐도 감동      1\n",
       "7   7865729                        개들의 전쟁2 나오나요? 나오면 1빠로 보고 싶음      1\n",
       "8   7477618                                                  굿      1\n",
       "9   9250537                                     바보가 아니라 병 쉰 인듯      1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
    "data = pd.read_table('ratings.txt') # 데이터 프레임에 저장\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 200000\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 : {}'.format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = data[:100] # 임의로 100개만 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규 표현식을 통해서 데이터를 정제합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohyeji/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2190435</td>\n",
       "      <td>사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9279041</td>\n",
       "      <td>완전 감동입니다 다시봐도 감동</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7865729</td>\n",
       "      <td>개들의 전쟁 나오나요 나오면 빠로 보고 싶음</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7477618</td>\n",
       "      <td>굿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9250537</td>\n",
       "      <td>바보가 아니라 병 쉰 인듯</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
       "2   4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
       "3   9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
       "4  10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1\n",
       "5   2190435                      사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화      1\n",
       "6   9279041                                   완전 감동입니다 다시봐도 감동      1\n",
       "7   7865729                           개들의 전쟁 나오나요 나오면 빠로 보고 싶음      1\n",
       "8   7477618                                                  굿      1\n",
       "9   9250537                                     바보가 아니라 병 쉰 인듯      1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data['document'] = sample_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
    "# 한글과 공백을 제외하고 모두 제거\n",
    "sample_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화를 수행해보겠습니다. \n",
    "\n",
    "__토큰화 과정에서 불용어를 제거하기 위해 불용어를 우선 정의합니다.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석기는 mecab을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for sentence in sample_data['document']:\n",
    "    temp = []\n",
    "    temp = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp = [word for word in temp if not word in stopwords] # 불용어 제거\n",
    "    tokenized.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ'], ['디자인', '을', '배우', '학생', '외국', '디자이너', '그', '일군', '전통', '을', '통해', '발전', '해', '문화', '산업', '부러웠', '는데', '사실', '우리', '나라', '에서', '그', '어려운', '시절', '끝', '까지', '열정', '을', '지킨', '노라노', '같', '전통', '있', '어', '저', '같', '사람', '꿈', '을', '꾸', '고', '이뤄나갈', '수', '있', '다는', '것', '감사', '합니다'], ['폴리스', '스토리', '시리즈', '부터', '뉴', '까지', '버릴', '께', '하나', '없', '음', '최고'], ['연기', '진짜', '개', '쩔', '구나', '지루', '할거', '라고', '생각', '했', '는데', '몰입', '해서', '봤', '다', '그래', '이런', '게', '진짜', '영화', '지'], ['안개', '자욱', '밤하늘', '떠', '있', '초승달', '같', '영화'], ['사랑', '을', '해', '본', '사람', '라면', '처음', '부터', '끝', '까지', '웃', '을', '수', '있', '영화'], ['완전', '감동', '입니다', '다시', '봐도', '감동'], ['개', '전쟁', '나오', '나요', '나오', '면', '빠', '로', '보', '고', '싶', '음'], ['굿'], ['바보', '아니', '라', '병', '쉰', '인', '듯']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 단어 집합을 만들어봅시다. __NLTK에서는 빈도수 계산 도구인 FreqDist()를 지원합니다.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 664\n"
     ]
    }
   ],
   "source": [
    "vocab = FreqDist(np.hstack(tokenized))\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어를 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장되어져 있습니다. vocab에 단어를 입력하면 빈도수를 리턴합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['재밌']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'재밌'이란 단어가 총 10번 등장하였습니다. most_common()는 상위 빈도수를 가진 주어진 수의 단어만을 리턴합니다. 이를 사용하여 등장 빈도수가 높은 단어들을 원하는 개수만큼만 얻을 수 있습니다. 등장 빈도수 상위 500개의 단어만 단어 집합으로 저장해봅시다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 500\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 500\n",
    "# 상위 vocab_size(=500)개의 단어만 보존\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 집합의 크기가 500으로 줄어든 것을 확인할 수 있습니다.\n",
    "\n",
    "***\n",
    "\n",
    "## 3. 각 단어에 고유한 정수 부여\n",
    "enumerate()는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스를 순차적으로 함께 리턴한다는 특징이 있습니다. \n",
    "\n",
    "__인덱스 0과 1은 다른 용도로 남겨두고 나머지 단어들은 2부터 501까지 순차적으로 인덱스를 부여해봅시다.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word[0]: index+2 for index, word in enumerate(vocab)}\n",
    "word_to_index['pad'] = 1 # pad에 인덱스 1 부여\n",
    "word_to_index['unk'] = 0 # unk에 인덱스 0 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 기존의 훈련 데이터에서 각 단어를 고유한 정수로 부여하는 작업을 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = []\n",
    "for line in tokenized: # 입력데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line: # 각 줄에서 1개씩 글자를 읽음\n",
    "        try:\n",
    "            temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "        except: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
    "            temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
    "    \n",
    "    encoded.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78, 27, 9, 4, 50, 41, 79, 16, 28, 29], [188, 5, 80, 189, 190, 191, 42, 192, 114, 5, 193, 194, 21, 115, 195, 196, 13, 51, 81, 116, 30, 42, 197, 117, 118, 31, 198, 5, 199, 200, 17, 114, 7, 82, 52, 17, 43, 201, 5, 202, 4, 203, 14, 7, 83, 32, 204, 84], [205, 119, 206, 53, 207, 31, 208, 209, 54, 10, 25, 11], [44, 33, 120, 210, 211, 212, 213, 68, 45, 34, 13, 214, 121, 15, 2, 215, 69, 8, 33, 3, 35], [216, 217, 218, 219, 7, 220, 17, 3], [122, 5, 21, 36, 43, 123, 124, 53, 118, 31, 85, 5, 14, 7, 3], [125, 37, 221, 41, 79, 37], [120, 222, 55, 223, 55, 86, 224, 46, 9, 4, 47, 25], [56], [225, 87, 88, 226, 227, 57, 89]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 4. 길이가 다른 문장들을 모두 동일한 길이로 바꿔주는 패딩(padding)\n",
    "이제 길이가 다른 리뷰들을 모두 동일한 길이로 바꿔주는 패딩 작업을 진행해보겠습니다. 앞서 단어 집합에 패딩을 위한 토큰인 'pad'를 추가했었습니다. 패딩 작업은 정해준 길이로 모든 샘플들의 길이를 맞춰주되, 길이가 정해준 길이보다 짧은 샘플들에는 'pad' 토큰을 추가하여 길이를 맞춰주는 작업입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 63\n",
      "리뷰의 최소 길이 : 1\n",
      "리뷰의 평균 길이 : 13.900000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = max(len(l) for l in encoded)\n",
    "print('리뷰의 최대 길이 : %d' % max_len)\n",
    "print('리뷰의 최소 길이 : %d' % min(len(l) for l in encoded))\n",
    "print('리뷰의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))\n",
    "\n",
    "plt.hist([len(s) for s in encoded], bins=50)\n",
    "plt.xlabel('length of sample')\n",
    "plt.ylabel('number of sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 길이가 긴 리뷰의 길이는 63입니다. 모든 리뷰의 길이를 63으로 통일시켜주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in encoded:\n",
    "    if len(line) < max_len:  # 현재 샘플이 정해준 길이보다 짧으면\n",
    "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 63\n",
      "리뷰의 최소 길이 : 63\n",
      "리뷰의 평균 길이 : 63.000000\n"
     ]
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 : %d' % max(len(l) for l in encoded))\n",
    "print('리뷰의 최소 길이 : %d' % min(len(l) for l in encoded))\n",
    "print('리뷰의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78, 27, 9, 4, 50, 41, 79, 16, 28, 29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [188, 5, 80, 189, 190, 191, 42, 192, 114, 5, 193, 194, 21, 115, 195, 196, 13, 51, 81, 116, 30, 42, 197, 117, 118, 31, 198, 5, 199, 200, 17, 114, 7, 82, 52, 17, 43, 201, 5, 202, 4, 203, 14, 7, 83, 32, 204, 84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [205, 119, 206, 53, 207, 31, 208, 209, 54, 10, 25, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 단어들을 고유한 정수로 맵핑하였으니, 각 정수를 고유한 단어 벡터로 바꾸는 작업이 필요합니다. 단어 벡터를 얻는 방법은 크게 원-핫 인코딩과 워드 임베딩이 있는데, 주로 워드 임베딩이 사용됩니다. 원-핫 인코딩과 워드 임베딩에 대해서는 9챕터에서 다룹니다.\n",
    "\n",
    "__9챕터로 넘어가기 전에 우리가 배워야 할 것은 방금 했던 과정을 좀 더 쉽게 수행해주는 파이토치의 토치텍스트라는 도구입니다.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
