{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. 다중 선형 회귀(Multivariable Linear regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 배운 x가 1개인 선형 회귀를 `단순 선형 회귀(Simple Linear Regression)`이라고 합니다.\n",
    "이번 챕터에서는 __다수의 x로부터 y를 예측하는 `다중 선형 회귀(Multivariable Linear Regression)`__에 대해서 이해합니다.\n",
    "\n",
    "\n",
    "$H(x)=w_1x_1+w_2x_2+w_3x_3+b$\n",
    "\n",
    "<img align='left' src='https://wikidocs.net/images/page/54841/%ED%9B%88%EB%A0%A8%EB%8D%B0%EC%9D%B4%ED%84%B0.PNG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # 신경망구축을위한 데이터구조, 레이어, loss 등\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim # 파라미터 최적화 알고리즘 (SGD 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1184323b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1) # 랜덤 시드 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식에서 x가 3개 이므로 x를 3개 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 가중치 $w$와 편향 $b$를 선언합니다. 가중치 $w$도 3개 선언해주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w와 편향 b 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 가설, cost function, 옵티마이저를 선언한 후에 경사 하강법(GD)을 1,000회 반복합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2 0.294 w3 0.297 b 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2 0.661 w3 0.676 b 0.008 Cost: 1.563634\n",
      "Epoch  200/1000 w1: 0.679 w2 0.655 w3 0.677 b 0.008 Cost: 1.497607\n",
      "Epoch  300/1000 w1: 0.684 w2 0.649 w3 0.677 b 0.008 Cost: 1.435026\n",
      "Epoch  400/1000 w1: 0.689 w2 0.643 w3 0.678 b 0.008 Cost: 1.375730\n",
      "Epoch  500/1000 w1: 0.694 w2 0.638 w3 0.678 b 0.009 Cost: 1.319511\n",
      "Epoch  600/1000 w1: 0.699 w2 0.633 w3 0.679 b 0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2 0.627 w3 0.679 b 0.009 Cost: 1.215696\n",
      "Epoch  800/1000 w1: 0.709 w2 0.622 w3 0.679 b 0.009 Cost: 1.167818\n",
      "Epoch  900/1000 w1: 0.713 w2 0.617 w3 0.680 b 0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2 0.613 w3 0.680 b 0.009 Cost: 1.079378\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train*w1 + x2_train*w2 + x3_train*w3 + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 초기화\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2 {:.3f} w3 {:.3f} b {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
    "        ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 벡터와 행렬 연산으로 바꾸기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드를 개선할 수 있는 부분이 있습니다. 이번에는 x의 개수가 3개였으니까 x1_train, x2_train, x3_train와 w1, w2, w3를 일일히 선언해주었습니다. 그런데 x의 개수가 1,000개라고 가정해봅시다. 위와 같은 방식을 고수할 경우 x_train1 ~ x_train1000을 전부 선언하고, w1 ~ w1000을 전부 선언해야 합니다. 다시 말해 x와 w 변수 선언만 총 합 2,000개를 해야합니다. 또한 가설을 선언하는 부분에서도 마찬가지로 x_train과 w의 곱셈이 이루어지는 항을 1,000개를 작성해야 합니다. 이는 굉장히 비효율적입니다.\n",
    "\n",
    "이를 해결하기 위해 행렬 곱셈 연산(또는 벡터의 내적)을 사용합니다.\n",
    "- 행렬의 곱셈 과정에서 이루어지는 벡터 연산을 `벡터의 내적(Dot Product)`이라고 합니다.\n",
    "\n",
    "\n",
    "<img align='left' src='https://wikidocs.net/images/page/54841/%ED%96%89%EB%A0%AC%EA%B3%B1.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림은 행렬 곱셈 연산 과정에서 벡터의 내적으로 1 × 7 + 2 × 9 + 3 × 11 = 58이 되는 과정을 보여줍니다.\n",
    "\n",
    "이 행렬 연산이 어떻게 현재 배우고 있는 가설과 상관이 있다는 걸까요?\n",
    "바로 가설을 벡터와 행렬 연산으로 표현할 수 있기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 벡터 연산으로 이해하기\n",
    "\n",
    "$H(x)=w_1x_1+w_2x_2+w_3x_3+b$\n",
    "\n",
    "위 식은 아래와 같이 두 벡터의 내적으로 표현할 수 있습니다.\n",
    "\n",
    "<img align='left' src='https://wikidocs.net/images/page/54841/%EB%82%B4%EC%A0%81.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 벡터를 각각 X와 W로 표현한다면, 가설은 다음과 같습니다.\n",
    "\n",
    "$H(X)=XW$\n",
    "\n",
    "\n",
    "x의 개수가 3개였음에도 이제는 X와 W라는 두 개의 변수로 표현된 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2. 행렬 연산으로 이해하기\n",
    "\n",
    "훈련 데이터를 살펴보고, 벡터와 행렬 연산을 통해 가설 H(X)를 표현해보겠습니다\n",
    "\n",
    "\n",
    "<img align='left' src='https://wikidocs.net/images/page/54841/%ED%9B%88%EB%A0%A8%EB%8D%B0%EC%9D%B4%ED%84%B0.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 훈련 데이터의 개수를 셀 수 있는 1개의 단위를 `샘플(sample)`이라고 합니다. 현재 샘플의 수는 총 5개입니다.\n",
    "각 샘플에서 y를 결정하게 하는 각각의 독립 변수 x를 `특성(feature)`이라고 합니다. 현재 특성은 3개입니다.\n",
    "\n",
    "이는 종속 변수 x들의 수가 (샘플의 수 × 특성의 수) = 15개임을 의미합니다. 종속 변수 x들을 (샘플의 수 × 특성의 수)의 크기를 가지는 하나의 행렬로 표현해봅시다. 그리고 이 행렬을 X라고 하겠습니다.\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "x11 & x12 & x13 \\\\\\\\ \n",
    "x21 & x22 & x23 \\\\\\\\ \n",
    "x31 & x32 & x33 \\\\\\\\ \n",
    "x41 & x42 & x43 \\\\\\\\ \n",
    "x51 & x52 & x53  \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 여기에 가중치 w1,w2,w3을 원소로 하는 벡터를 W라 하고 이를 곱해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "x11 & x12 & x13 \\\\\\\\ \n",
    "x21 & x22 & x23 \\\\\\\\ \n",
    "x31 & x32 & x33 \\\\\\\\ \n",
    "x41 & x42 & x43 \\\\\\\\ \n",
    "x51 & x52 & x53  \n",
    "\\end{bmatrix}$\n",
    "$\\begin{bmatrix}\n",
    "w1 \\\\\\\\ \n",
    "w2 \\\\\\\\ \n",
    "w3  \n",
    "\\end{bmatrix}$\n",
    "$=$\n",
    "$\\begin{bmatrix}\n",
    "x11w1 + x12w2 + x13w3 \\\\\\\\ \n",
    "x21w1 + x22w2 + x23w3 \\\\\\\\ \n",
    "x31w1 + x32w2 + x33w3 \\\\\\\\ \n",
    "x41w1 + x42w2 + x43w3 \\\\\\\\ \n",
    "x51w1 + x52w2 + x53w3  \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식은 결과적으로 다음과 같습니다.\n",
    "\n",
    "$H(X)=XW$\n",
    "\n",
    "이 가설에 각 샘플에 더해지는 편향 $b$를 추가해봅시다. 샘플 수만큼의 차원을 가지는 편향 벡터 $B$를 만들어 더합니다.\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "x11 & x12 & x13 \\\\\\\\ \n",
    "x21 & x22 & x23 \\\\\\\\ \n",
    "x31 & x32 & x33 \\\\\\\\ \n",
    "x41 & x42 & x43 \\\\\\\\ \n",
    "x51 & x52 & x53  \n",
    "\\end{bmatrix}$\n",
    "$\\begin{bmatrix}\n",
    "w1 \\\\\\\\ \n",
    "w2 \\\\\\\\ \n",
    "w3  \n",
    "\\end{bmatrix}$\n",
    "$+$\n",
    "$\\begin{bmatrix}\n",
    "b \\\\\\\\ \n",
    "b \\\\\\\\ \n",
    "b \\\\\\\\ \n",
    "b \\\\\\\\ \n",
    "b\n",
    "\\end{bmatrix}$\n",
    "$=$\n",
    "$\\begin{bmatrix}\n",
    "x11w1 + x12w2 + x13w3 + b \\\\\\\\ \n",
    "x21w1 + x22w2 + x23w3 + b \\\\\\\\ \n",
    "x31w1 + x32w2 + x33w3 + b \\\\\\\\ \n",
    "x41w1 + x42w2 + x43w3 + b \\\\\\\\ \n",
    "x51w1 + x52w2 + x53w3 + b  \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식은 결과적으로 다음과 같습니다.\n",
    "\n",
    "$H(X)=XW+B$\n",
    "\n",
    "\n",
    "결과적으로 전체 훈련 데이터의 가설 연산을 3개의 변수만으로 표현하였습니다.\n",
    "이와 같이 벡터와 행렬 연산은 식을 간단하게 해줄 뿐만 아니라 다수의 샘플의 병렬 연산이므로 속도의 이점을 가집니다.\n",
    "\n",
    "이를 참고로 파이토치로 구현해봅시다.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 행렬 연산을 고려하여 파이토치로 구현하기\n",
    "\n",
    "이번에는 행렬 연산을 고려하여 파이토치로 재구현해보겠습니다.\n",
    "이번에는 훈련 데이터 또한 행렬로 선언해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([\n",
    "    [73, 80, 75],\n",
    "    [93, 88, 93],\n",
    "    [89, 91, 90],\n",
    "    [96, 98, 100],\n",
    "    [73, 66, 70]\n",
    "])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전에 x_train을 3개나 구현했던 것과 다르게 이번에는 x_train 하나에 모든 샘플을 전부 선언하였습니다. 다시 말해 (5 x 3) 행렬 X을 선언한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각 (5 × 3) 행렬과 (5 × 1) 행렬(또는 벡터)의 크기를 가집니다.\n",
    "이제 가중치 W와 편향 b를 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치와 편향 선언\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 주목할 점은 가중치 W의 크기가 (3 × 1) 벡터라는 점입니다. 행렬의 곱셈이 성립되려면 곱셈의 좌측에 있는 행렬의 열의 크기와 우측에 있는 행렬의 행의 크기가 일치해야 합니다. 현재 X_train의 행렬의 크기는 (5 × 3)이며, W 벡터의 크기는 (3 × 1)이므로 두 행렬과 벡터는 행렬곱이 가능합니다. 행렬곱으로 가설을 선언하면 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = x_train.matmul(W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가설을 행렬곱으로 간단히 정의하였습니다. 이는 앞서 x_train과 w의 곱셈이 이루어지는 각 항을 전부 기재하여 가설을 선언했던 것과 대비됩니다. 이 경우, 사용자가 독립 변수 x의 수를 후에 추가적으로 늘리거나 줄이더라도 위의 가설 선언 코드를 수정할 필요가 없습니다. 이제 해야할 일은 비용 함수와 옵티마이저를 정의하고, 정해진 에포크만큼 훈련을 진행하는 일입니다. 이를 반영한 전체 코드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
      "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n",
      "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n",
      "Epoch    4/20 hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936005\n",
      "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371010\n",
      "Epoch    6/20 hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]) Cost: 29.758139\n",
      "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]) Cost: 10.445305\n",
      "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391228\n",
      "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493135\n",
      "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
      "Epoch   11/20 hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]) Cost: 1.710541\n",
      "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651412\n",
      "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632387\n",
      "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625923\n",
      "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623412\n",
      "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622141\n",
      "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621253\n",
      "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]) Cost: 1.620500\n",
      "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]) Cost: 1.619770\n",
      "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.1000]) Cost: 1.619033\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([\n",
    "    [73, 80, 75],\n",
    "    [93, 88, 93],\n",
    "    [89, 91, 90],\n",
    "    [96, 98, 100],\n",
    "    [73, 66, 70]\n",
    "])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer= optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # H(x) 계산\n",
    "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # gradient를 0으로 초기화\n",
    "    cost.backward() # 비용 함수를 미분하여 gradient 계산\n",
    "    optimizer.step() # W와 b를 업데이트\n",
    "    \n",
    "#     참고) \n",
    "#     스퀴즈(Squeeze) - 1인 차원을 제거한다.\n",
    "#     .detach() 를 호출하여 내용물(content)은 같지만 require_grad가 다른 새로운 Tensor를 가져옵니다\n",
    "#     print(hypothesis.requires_grad) #  requires_grad: True => trainable 변수. 이는 이 변수는 학습을 통해 계속 값이 변경되는 변수임을 의미합니다.\n",
    "#     test = hypothesis.detach().requires_grad # => False\n",
    "#     print(test) : \n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
