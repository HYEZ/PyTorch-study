{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. nn.Module로 구현하는 로지스틱 회귀\n",
    "\n",
    "잠깐만 복습을 해보면 선형 회귀 모델의 가설식은 H(x)=Wx+b이었습니다. 그리고 이 가설식을 구현하기 위해서 파이토치의 nn.Linear()를 사용했습니다. 그리고 로지스틱 회귀의 가설식은 H(x)=sigmoid(Wx+b)입니다. 파이토치에서는 nn.Sigmoid()를 통해서 시그모이드 함수를 구현하므로 결과적으로 nn.Linear()의 결과를 nn.Sigmoid()를 거치게하면 로지스틱 회귀의 가설식이 됩니다.\n",
    "\n",
    "파이토치를 통해 이를 구현해봅시다.\n",
    "\n",
    "## 1. 파이토치의 nn.Linear와 nn.Sigmoid로 로지스틱 회귀 구현하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10af0f610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련데이터를 텐서로 선언\n",
    "x_train = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_train = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.FloatTensor(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential()은 nn.Module 층을 차례로 쌓을 수 있도록 합니다. 뒤에서 이를 이용해서 인공 신경망을 구현하게 되므로 기억하고 있으면 좋습니다. 조금 쉽게 말해서 nn.Sequential()은 Wx+b와 같은 수식과 시그모이드 함수 등과 같은 여러 함수들을 연결해주는 역할을 합니다. 이를 이용해서 로지스틱 회귀를 구현해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 1), # input_dim = 2, output_dim = 1\n",
    "    nn.Sigmoid() # 출력은 시그모이드 함수를 거친다\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 W와 b는 랜덤 초기화가 된 상태입니다. 훈련 데이터를 넣어 예측값을 확인해봅시다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4020],\n",
       "        [0.4147],\n",
       "        [0.6556],\n",
       "        [0.5948],\n",
       "        [0.6788],\n",
       "        [0.8061]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 × 1 크기의 예측값 텐서가 출력됩니다. 그러나 현재 W와 b는 임의의 값을 가지므로 현재의 예측은 의미가 없습니다.\n",
    "이제 경사 하강법을 사용하여 훈련해보겠습니다. 총 100번의 에포크를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.539713 Accuracy: 83.33%\n",
      "Epoch   10/1000 Cost: 0.614853 Accuracy: 66.67%\n",
      "Epoch   20/1000 Cost: 0.441875 Accuracy: 66.67%\n",
      "Epoch   30/1000 Cost: 0.373145 Accuracy: 83.33%\n",
      "Epoch   40/1000 Cost: 0.316358 Accuracy: 83.33%\n",
      "Epoch   50/1000 Cost: 0.266094 Accuracy: 83.33%\n",
      "Epoch   60/1000 Cost: 0.220498 Accuracy: 100.00%\n",
      "Epoch   70/1000 Cost: 0.182095 Accuracy: 100.00%\n",
      "Epoch   80/1000 Cost: 0.157299 Accuracy: 100.00%\n",
      "Epoch   90/1000 Cost: 0.144091 Accuracy: 100.00%\n",
      "Epoch  100/1000 Cost: 0.134272 Accuracy: 100.00%\n",
      "Epoch  110/1000 Cost: 0.125769 Accuracy: 100.00%\n",
      "Epoch  120/1000 Cost: 0.118297 Accuracy: 100.00%\n",
      "Epoch  130/1000 Cost: 0.111680 Accuracy: 100.00%\n",
      "Epoch  140/1000 Cost: 0.105779 Accuracy: 100.00%\n",
      "Epoch  150/1000 Cost: 0.100483 Accuracy: 100.00%\n",
      "Epoch  160/1000 Cost: 0.095704 Accuracy: 100.00%\n",
      "Epoch  170/1000 Cost: 0.091369 Accuracy: 100.00%\n",
      "Epoch  180/1000 Cost: 0.087420 Accuracy: 100.00%\n",
      "Epoch  190/1000 Cost: 0.083806 Accuracy: 100.00%\n",
      "Epoch  200/1000 Cost: 0.080486 Accuracy: 100.00%\n",
      "Epoch  210/1000 Cost: 0.077425 Accuracy: 100.00%\n",
      "Epoch  220/1000 Cost: 0.074595 Accuracy: 100.00%\n",
      "Epoch  230/1000 Cost: 0.071969 Accuracy: 100.00%\n",
      "Epoch  240/1000 Cost: 0.069526 Accuracy: 100.00%\n",
      "Epoch  250/1000 Cost: 0.067248 Accuracy: 100.00%\n",
      "Epoch  260/1000 Cost: 0.065118 Accuracy: 100.00%\n",
      "Epoch  270/1000 Cost: 0.063122 Accuracy: 100.00%\n",
      "Epoch  280/1000 Cost: 0.061247 Accuracy: 100.00%\n",
      "Epoch  290/1000 Cost: 0.059483 Accuracy: 100.00%\n",
      "Epoch  300/1000 Cost: 0.057820 Accuracy: 100.00%\n",
      "Epoch  310/1000 Cost: 0.056250 Accuracy: 100.00%\n",
      "Epoch  320/1000 Cost: 0.054764 Accuracy: 100.00%\n",
      "Epoch  330/1000 Cost: 0.053357 Accuracy: 100.00%\n",
      "Epoch  340/1000 Cost: 0.052022 Accuracy: 100.00%\n",
      "Epoch  350/1000 Cost: 0.050753 Accuracy: 100.00%\n",
      "Epoch  360/1000 Cost: 0.049546 Accuracy: 100.00%\n",
      "Epoch  370/1000 Cost: 0.048396 Accuracy: 100.00%\n",
      "Epoch  380/1000 Cost: 0.047299 Accuracy: 100.00%\n",
      "Epoch  390/1000 Cost: 0.046252 Accuracy: 100.00%\n",
      "Epoch  400/1000 Cost: 0.045251 Accuracy: 100.00%\n",
      "Epoch  410/1000 Cost: 0.044294 Accuracy: 100.00%\n",
      "Epoch  420/1000 Cost: 0.043376 Accuracy: 100.00%\n",
      "Epoch  430/1000 Cost: 0.042497 Accuracy: 100.00%\n",
      "Epoch  440/1000 Cost: 0.041653 Accuracy: 100.00%\n",
      "Epoch  450/1000 Cost: 0.040843 Accuracy: 100.00%\n",
      "Epoch  460/1000 Cost: 0.040064 Accuracy: 100.00%\n",
      "Epoch  470/1000 Cost: 0.039315 Accuracy: 100.00%\n",
      "Epoch  480/1000 Cost: 0.038593 Accuracy: 100.00%\n",
      "Epoch  490/1000 Cost: 0.037898 Accuracy: 100.00%\n",
      "Epoch  500/1000 Cost: 0.037228 Accuracy: 100.00%\n",
      "Epoch  510/1000 Cost: 0.036582 Accuracy: 100.00%\n",
      "Epoch  520/1000 Cost: 0.035958 Accuracy: 100.00%\n",
      "Epoch  530/1000 Cost: 0.035356 Accuracy: 100.00%\n",
      "Epoch  540/1000 Cost: 0.034773 Accuracy: 100.00%\n",
      "Epoch  550/1000 Cost: 0.034210 Accuracy: 100.00%\n",
      "Epoch  560/1000 Cost: 0.033664 Accuracy: 100.00%\n",
      "Epoch  570/1000 Cost: 0.033137 Accuracy: 100.00%\n",
      "Epoch  580/1000 Cost: 0.032625 Accuracy: 100.00%\n",
      "Epoch  590/1000 Cost: 0.032130 Accuracy: 100.00%\n",
      "Epoch  600/1000 Cost: 0.031649 Accuracy: 100.00%\n",
      "Epoch  610/1000 Cost: 0.031183 Accuracy: 100.00%\n",
      "Epoch  620/1000 Cost: 0.030730 Accuracy: 100.00%\n",
      "Epoch  630/1000 Cost: 0.030291 Accuracy: 100.00%\n",
      "Epoch  640/1000 Cost: 0.029864 Accuracy: 100.00%\n",
      "Epoch  650/1000 Cost: 0.029449 Accuracy: 100.00%\n",
      "Epoch  660/1000 Cost: 0.029046 Accuracy: 100.00%\n",
      "Epoch  670/1000 Cost: 0.028654 Accuracy: 100.00%\n",
      "Epoch  680/1000 Cost: 0.028272 Accuracy: 100.00%\n",
      "Epoch  690/1000 Cost: 0.027900 Accuracy: 100.00%\n",
      "Epoch  700/1000 Cost: 0.027538 Accuracy: 100.00%\n",
      "Epoch  710/1000 Cost: 0.027186 Accuracy: 100.00%\n",
      "Epoch  720/1000 Cost: 0.026842 Accuracy: 100.00%\n",
      "Epoch  730/1000 Cost: 0.026507 Accuracy: 100.00%\n",
      "Epoch  740/1000 Cost: 0.026181 Accuracy: 100.00%\n",
      "Epoch  750/1000 Cost: 0.025862 Accuracy: 100.00%\n",
      "Epoch  760/1000 Cost: 0.025552 Accuracy: 100.00%\n",
      "Epoch  770/1000 Cost: 0.025248 Accuracy: 100.00%\n",
      "Epoch  780/1000 Cost: 0.024952 Accuracy: 100.00%\n",
      "Epoch  790/1000 Cost: 0.024663 Accuracy: 100.00%\n",
      "Epoch  800/1000 Cost: 0.024381 Accuracy: 100.00%\n",
      "Epoch  810/1000 Cost: 0.024104 Accuracy: 100.00%\n",
      "Epoch  820/1000 Cost: 0.023835 Accuracy: 100.00%\n",
      "Epoch  830/1000 Cost: 0.023571 Accuracy: 100.00%\n",
      "Epoch  840/1000 Cost: 0.023313 Accuracy: 100.00%\n",
      "Epoch  850/1000 Cost: 0.023061 Accuracy: 100.00%\n",
      "Epoch  860/1000 Cost: 0.022814 Accuracy: 100.00%\n",
      "Epoch  870/1000 Cost: 0.022572 Accuracy: 100.00%\n",
      "Epoch  880/1000 Cost: 0.022336 Accuracy: 100.00%\n",
      "Epoch  890/1000 Cost: 0.022104 Accuracy: 100.00%\n",
      "Epoch  900/1000 Cost: 0.021877 Accuracy: 100.00%\n",
      "Epoch  910/1000 Cost: 0.021655 Accuracy: 100.00%\n",
      "Epoch  920/1000 Cost: 0.021437 Accuracy: 100.00%\n",
      "Epoch  930/1000 Cost: 0.021224 Accuracy: 100.00%\n",
      "Epoch  940/1000 Cost: 0.021015 Accuracy: 100.00%\n",
      "Epoch  950/1000 Cost: 0.020810 Accuracy: 100.00%\n",
      "Epoch  960/1000 Cost: 0.020609 Accuracy: 100.00%\n",
      "Epoch  970/1000 Cost: 0.020412 Accuracy: 100.00%\n",
      "Epoch  980/1000 Cost: 0.020219 Accuracy: 100.00%\n",
      "Epoch  990/1000 Cost: 0.020029 Accuracy: 100.00%\n",
      "Epoch 1000/1000 Cost: 0.019843 Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 10번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy: {:2.2f}%'.format(\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중간부터 정확도는 100%가 나오기 시작합니다. 기존의 훈련 데이터를 입력하여 예측값을 확인해보겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7616e-04],\n",
       "        [3.1595e-02],\n",
       "        [3.8959e-02],\n",
       "        [9.5624e-01],\n",
       "        [9.9823e-01],\n",
       "        [9.9969e-01]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5를 넘으면 True, 그보다 낮으면 False로 간주합니다. 실제값은 [[0], [0], [0], [1], [1], [1]]입니다. 이는 False, False, False, True, True, True에 해당되므로 전부 실제값과 일치하도록 예측한 것을 확인할 수 있습니다.\n",
    "\n",
    "훈련 후의 W와 b의 값을 출력해보겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[3.2534, 1.5181]], requires_grad=True), Parameter containing:\n",
      "tensor([-14.4839], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력값이 앞 챕터에서 nn.Module을 사용하지 않고 로지스틱 회귀를 구현한 실습에서 얻었던 W와 b와 거의 일치합니다.\n",
    "\n",
    "***\n",
    "\n",
    "## 2. 인공 신경망으로 표현되는 로지스틱 회귀.\n",
    "사실 로지스틱 회귀는 인공 신경망으로 간주할 수 있습니다.\n",
    "\n",
    "<img align='left' src='https://wikidocs.net/images/page/58686/logistic_regression.PNG'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 인공 신경망 그림에서 각 화살표는 입력과 곱해지는 가중치 또는 편향입니다. 각 입력에 대해서 검은색 화살표는 가중치, 회색 화살표는 편향이 곱해집니다. 각 입력 x는 각 입력의 가중치 w와 곱해지고, 편향 b는 상수 1과 곱해지는 것으로 표현되었습니다. 그리고 출력하기 전에 시그모이드 함수를 지나게 됩니다.\n",
    "\n",
    "결과적으로 위의 인공 신경망은 다음과 같은 다중 로지스틱 회귀를 표현하고 있습니다.\n",
    "\n",
    "$H(x)=sigmoid(x_1w_1+x_2w_2+b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
